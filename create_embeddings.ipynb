{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Q&A data for search\n",
    "\n",
    "Preparing dataset Q&A sessions\n",
    "\n",
    "Procedure:\n",
    "\n",
    "0. Prerequisites: Import libraries, set API key (if needed)\n",
    "1. Load: Load Q&A data from document text files\n",
    "2. Chunk: Each file is split into tuples of (question, answer) pairs\n",
    "3. Embed: Each section is embedded with the OpenAI API\n",
    "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import openai  # for generating embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens\n",
    "import matplotlib.pyplot as plt # for basic plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai`.)\n",
    "\n",
    "If you install any libraries, be sure to restart the notebook kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API key (if needed)\n",
    "\n",
    "Note that the OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "org-jtv5LwV6WqynnSQzFQQR3xR3\n"
     ]
    }
   ],
   "source": [
    "!echo $OPENAI_API_KEY\n",
    "!echo $OPENAI_ORGANIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = os.getenv(\"OPENAI_ORGANIZATION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 sections\n"
     ]
    }
   ],
   "source": [
    "# get data from .txt file\n",
    "\n",
    "FILE_NAME = \"db/raw_convo.txt\"\n",
    "\n",
    "\n",
    "def split_into_sections(file_name: str) -> [tuple[str, [str]]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    file = open(FILE_NAME, \"r\")\n",
    "    res, tmp, active, match_tok = [], None, False, 0\n",
    "    \n",
    "    for line in file.readlines():\n",
    "        if line.strip().isnumeric() and int(line) == match_tok+1:\n",
    "            if tmp:\n",
    "                res.append((str(match_tok), tmp))\n",
    "            \n",
    "            match_tok += 1\n",
    "            active = True\n",
    "            tmp = []\n",
    "            continue\n",
    "        \n",
    "        if active:\n",
    "            tmp.append(line)\n",
    "            \n",
    "    file.close()\n",
    "            \n",
    "    return res\n",
    "\n",
    "def save_sections_to_new_files(sections: [tuple[str, [str]]]) -> None:\n",
    "    \"\"\"\n",
    "    Writes extracted sections to new individual text files for easier future processing\n",
    "    \"\"\"\n",
    "    for title, text in sections:\n",
    "        f = open(f\"db/{title}.txt\", \"w\")\n",
    "        for line in text:\n",
    "            f.write(line)\n",
    "    \n",
    "sections = split_into_sections(FILE_NAME)\n",
    "# save_sections_to_new_files(sections)\n",
    "\n",
    "print(\"Found %d sections\" % len(sections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1',\n",
       "  [(['My one-year-old has a fever of 40. Is it dangerous?\\n'],\n",
       "    'Many parents think that if the temperature is very high, the cause of the fever must be more serious. How your child looks is much more important than the exact number of the temperature. When the temperature is high, your child might look more sick. When you give your child fever medicine, your child should feel better and be more active. If your child looks well once the temperature is down, we are usually reassured that there is not a serious infection.\\n'),\n",
       "   (['Is a temperature of 40 dangerous?\\n'],\n",
       "    'When your child has a fever, the most important thing is how your child looks and acts, not the number on the thermometer. For example, when the fever is high, your child will probably look more sick and feel very tired. When the fever is better, your child\\u200a should be more active, playful and able to drink fluids.\\nMany parents worry if their child’s temperature is higher, for example, 39° or 40°, that it could mean something more dangerous is going on. Many children with a cold or flu develop a fever of 39° or 40°.\\n'),\n",
       "   (['Could a fever cause brain damage? \\n'],\n",
       "    \"Fevers are a sign that your child's immune system is fighting an infection. Fevers from a cold or flu do not cause brain damage. Fevers do not cause damage to your child’s brain, body or organs.\\n\")])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_and_join_ans(ans: [str]) -> str:\n",
    "    ans = [tok for tok in ans if tok not in ['\\n', '\\t', '\\u2003\\n']]\n",
    "    return ''.join(ans)\n",
    "\n",
    "def split_into_qa_pairs(sections: [tuple[str, [str]]]) -> [tuple[[str], str]]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    res, questions, ans = [], [], []\n",
    "    for section, text in sections:\n",
    "        tmp = []\n",
    "        for line in text:\n",
    "            if line.startswith(\"<Q>\"):\n",
    "                if len(ans) > 0:\n",
    "                    p_ans = parse_and_join_ans(ans)\n",
    "                    tmp.append((questions, p_ans))\n",
    "                    questions = []\n",
    "                    ans = []\n",
    "                \n",
    "                questions.append(line[4:])\n",
    "                continue\n",
    "            \n",
    "            ans.append(line)\n",
    "            \n",
    "        if len(ans) > 0:\n",
    "            p_ans = parse_and_join_ans(ans)\n",
    "            tmp.append((questions, p_ans))\n",
    "            \n",
    "        res.append((section, tmp))\n",
    "        \n",
    "    return res\n",
    "\n",
    "qa_sections = split_into_qa_pairs(sections)\n",
    "qa_sections[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "- Discard less relevant-looking sections like External Links and Footnotes\n",
    "- Clean up the text by removing reference tags (e.g., <ref>), whitespace, and super short sections\n",
    "- Split each article into sections\n",
    "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My one-year-old has a fever of 40. Is it dangerous?\\n']\n",
      "Many parents think that if the temperature is very high, the cause of the fev...\n",
      "\n",
      "['Is a temperature of 40 dangerous?\\n']\n",
      "When your child has a fever, the most important thing is how your child looks...\n",
      "\n",
      "['Could a fever cause brain damage? \\n']\n",
      "Fevers are a sign that your child's immune system is fighting an infection. F...\n",
      "\n",
      "['Could a fever cause brain damage? \\n']\n",
      "Fevers are a sign that your child's immune system is fighting an infection. F...\n",
      "\n",
      "['I keep giving Tylenol but my child fever doesn’t go down.\\n']\n",
      "Fever medicine will lower the temperature a little and will make your child m...\n",
      "\n",
      "['The fever is going up and down and keeps coming back\\n']\n",
      "It is normal for your child’s temperature to go up and down through the day a...\n",
      "\n",
      "['The fever is going up and down and keeps coming back\\n']\n",
      "It is normal for your child’s temperature to go up and down through the day a...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For now, we don't include section title in the question answer data and just collect all subsections in one array\n",
    "# TODO: Add a section title instead of simple index (1, 2, 3, ...) that describes the \n",
    "#       types of questions asked in that block and prepend to question array\n",
    "\n",
    "qa_data = []\n",
    "\n",
    "for section, data in qa_sections:\n",
    "    for questions, answer in data:\n",
    "        qa_data.append((questions, answer))\n",
    "\n",
    "for qs, a in qa_data[:7]:\n",
    "    print(qs)\n",
    "    print(a[:77] + \"...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "\n",
    "Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # no delimiter found\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # no need to search for halfway point\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str,\n",
    "    max_tokens: int,\n",
    "    print_warning: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "    subsection: tuple[list[str], str],\n",
    "    max_tokens: int = 1000,\n",
    "    model: str = GPT_MODEL,\n",
    "    max_recursion: int = 5,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
    "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
    "    \"\"\"\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    num_tokens_in_string = num_tokens(string)\n",
    "    # if length is fine, return string\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "    # if recursion hasn't found a split after X iterations, just truncate\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    # otherwise, split in half and recurse\n",
    "    else:\n",
    "        titles, text = subsection\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                # if either half is empty, retry with a more fine-grained delimiter\n",
    "                continue\n",
    "            else:\n",
    "                # recurse on each half\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_subsection = (titles, half)\n",
    "                    half_strings = split_strings_from_subsection(\n",
    "                        half_subsection,\n",
    "                        max_tokens=max_tokens,\n",
    "                        model=model,\n",
    "                        max_recursion=max_recursion - 1,\n",
    "                    )\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    # otherwise no split was found, so just truncate (should be very rare)\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfp0lEQVR4nO3df1BVdf7H8Rc/5ILhBUG9VxLM1lYsf2xh6i1rW2NjXddqZXbLsSLXqalFV6UtY9tya9fFqdnsx6C1jeHsbEY5k7Y6ZeNi4ToBKkmpFenmBhteXDNALS4En+8fbffbDXK9cvlcLzwfM2cmzjkc3vezizzncg83yhhjBAAAYEl0uAcAAAD9C/EBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq2LDPcA3dXZ2qqGhQYMGDVJUVFS4xwEAAKfBGKPjx48rLS1N0dGnfm7jrIuPhoYGpaenh3sMAABwBurr6zVixIhTnnPWxcegQYMkfTm80+kM8zQAAOB0tLS0KD093f9z/FTOuvj46lctTqeT+AAAIMKczksmeMEpAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKvOune17XV1ddLRo+GeIjhDhkgZGeGeAgCAkOhf8VFXp47MsYr5/LNwTxKUjoSBinn/PQIEANAn9K/4OHpUMZ9/pkU/uUsHU9PDPc1pGf1JvR7f/Kcvn60hPgAAfUD/io//Opiarv3u0eEeAwCAfokXnAIAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVUHFx+9+9ztFRUUFbJmZmf7jra2tys/PV2pqqhITE5Wbm6vGxsaQDw0AACJX0M98XHTRRTp8+LB/27Fjh//YkiVLtGnTJq1fv17l5eVqaGjQ7NmzQzowAACIbLFBf0JsrNxud5f9zc3NWrNmjdatW6fp06dLkkpKSjR27FhVVlZq6tSpPZ8WAABEvKCf+Thw4IDS0tJ0/vnna+7cuaqrq5MkVVdXq729XdnZ2f5zMzMzlZGRoYqKitBNDAAAIlpQz3xMmTJFa9eu1ZgxY3T48GE9+OCDuuKKK7Rv3z55vV7FxcUpOTk54HNcLpe8Xu+3XtPn88nn8/k/bmlpCe4RAACAiBJUfMyYMcP/3xMmTNCUKVM0cuRIvfjii0pISDijAYqKivTggw+e0ecCAIDI06NbbZOTk/Xd735XBw8elNvtVltbm5qamgLOaWxs7PY1Il8pLCxUc3Ozf6uvr+/JSAAA4CzXo/g4ceKE/vnPf2r48OHKysrSgAEDVFZW5j9eW1ururo6eTyeb72Gw+GQ0+kM2AAAQN8V1K9dfv3rX2vWrFkaOXKkGhoatGzZMsXExGjOnDlKSkrS/PnzVVBQoJSUFDmdTi1cuFAej4c7XQAAgF9Q8fHvf/9bc+bM0SeffKKhQ4dq2rRpqqys1NChQyVJK1euVHR0tHJzc+Xz+ZSTk6NVq1b1yuAAACAyBRUfpaWlpzweHx+v4uJiFRcX92goAADQd/HeLgAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq3oUHytWrFBUVJQWL17s39fa2qr8/HylpqYqMTFRubm5amxs7OmcAACgjzjj+Ni1a5eefvppTZgwIWD/kiVLtGnTJq1fv17l5eVqaGjQ7NmzezwoAADoG84oPk6cOKG5c+fqmWee0eDBg/37m5ubtWbNGj366KOaPn26srKyVFJSojfffFOVlZUhGxoAAESuM4qP/Px8zZw5U9nZ2QH7q6ur1d7eHrA/MzNTGRkZqqio6PZaPp9PLS0tARsAAOi7YoP9hNLSUr311lvatWtXl2Ner1dxcXFKTk4O2O9yueT1eru9XlFRkR588MFgxwAAABEqqGc+6uvrtWjRIj333HOKj48PyQCFhYVqbm72b/X19SG5LgAAODsFFR/V1dU6cuSILrnkEsXGxio2Nlbl5eV64oknFBsbK5fLpba2NjU1NQV8XmNjo9xud7fXdDgccjqdARsAAOi7gvq1y9VXX629e/cG7Js3b54yMzO1dOlSpaena8CAASorK1Nubq4kqba2VnV1dfJ4PKGbGgAARKyg4mPQoEEaN25cwL5zzjlHqamp/v3z589XQUGBUlJS5HQ6tXDhQnk8Hk2dOjV0UwMAgIgV9AtO/5eVK1cqOjpaubm58vl8ysnJ0apVq0L9ZQAAQITqcXy88cYbAR/Hx8eruLhYxcXFPb00AADog3hvFwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVcQHAACwivgAAABWER8AAMAq4gMAAFhFfAAAAKuIDwAAYBXxAQAArCI+AACAVUHFx+rVqzVhwgQ5nU45nU55PB69+uqr/uOtra3Kz89XamqqEhMTlZubq8bGxpAPDQAAIldQ8TFixAitWLFC1dXV2r17t6ZPn67rrrtO+/fvlyQtWbJEmzZt0vr161VeXq6GhgbNnj27VwYHAACRKTaYk2fNmhXw8fLly7V69WpVVlZqxIgRWrNmjdatW6fp06dLkkpKSjR27FhVVlZq6tSpoZsaAABErDN+zUdHR4dKS0t18uRJeTweVVdXq729XdnZ2f5zMjMzlZGRoYqKim+9js/nU0tLS8AGAAD6rqDjY+/evUpMTJTD4dAdd9yhDRs26MILL5TX61VcXJySk5MDzne5XPJ6vd96vaKiIiUlJfm39PT0oB8EAACIHEHHx5gxY1RTU6OqqirdeeedysvL07vvvnvGAxQWFqq5udm/1dfXn/G1AADA2S+o13xIUlxcnEaPHi1JysrK0q5du/T444/rhhtuUFtbm5qamgKe/WhsbJTb7f7W6zkcDjkcjuAnBwAAEanHf+ejs7NTPp9PWVlZGjBggMrKyvzHamtrVVdXJ4/H09MvAwAA+oignvkoLCzUjBkzlJGRoePHj2vdunV644039NprrykpKUnz589XQUGBUlJS5HQ6tXDhQnk8Hu50AQAAfkHFx5EjR3TLLbfo8OHDSkpK0oQJE/Taa6/phz/8oSRp5cqVio6OVm5urnw+n3JycrRq1apeGRwAAESmoOJjzZo1pzweHx+v4uJiFRcX92goAADQd/HeLgAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwKrYcA+A0/Tee+GeIDhDhkgZGeGeAgBwFiI+znJDT3yqjqgoxdx0U7hHCUpHwkDFvP8eAQIA6IL4OMs5fScUY4wW/eQuHUxND/c4p2X0J/V6fPOfpKNHiQ8AQBfER4Q4mJqu/e7R4R4DAIAe4wWnAADAKuIDAABYRXwAAACriA8AAGAV8QEAAKwiPgAAgFVBxUdRUZEuvfRSDRo0SMOGDdP111+v2tragHNaW1uVn5+v1NRUJSYmKjc3V42NjSEdGgAARK6g4qO8vFz5+fmqrKzU1q1b1d7ermuuuUYnT570n7NkyRJt2rRJ69evV3l5uRoaGjR79uyQDw4AACJTUH9kbMuWLQEfr127VsOGDVN1dbWuvPJKNTc3a82aNVq3bp2mT58uSSopKdHYsWNVWVmpqVOnhm5yAAAQkXr0mo/m5mZJUkpKiiSpurpa7e3tys7O9p+TmZmpjIwMVVRUdHsNn8+nlpaWgA0AAPRdZxwfnZ2dWrx4sS6//HKNGzdOkuT1ehUXF6fk5OSAc10ul7xeb7fXKSoqUlJSkn9LT4+M9y8BAABn5ozjIz8/X/v27VNpaWmPBigsLFRzc7N/q6+v79H1AADA2e2M3lhuwYIF2rx5s7Zv364RI0b497vdbrW1tampqSng2Y/Gxka53e5ur+VwOORwOM5kDAAAEIGCeubDGKMFCxZow4YN2rZtm0aNGhVwPCsrSwMGDFBZWZl/X21trerq6uTxeEIzMQAAiGhBPfORn5+vdevW6eWXX9agQYP8r+NISkpSQkKCkpKSNH/+fBUUFCglJUVOp1MLFy6Ux+PhThcAACApyPhYvXq1JOmqq64K2F9SUqJbb71VkrRy5UpFR0crNzdXPp9POTk5WrVqVUiGBQAAkS+o+DDG/M9z4uPjVVxcrOLi4jMeCgAA9F28twsAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYFHR/bt2/XrFmzlJaWpqioKG3cuDHguDFGDzzwgIYPH66EhARlZ2frwIEDoZoXAABEuKDj4+TJk5o4caKKi4u7Pf7www/riSee0FNPPaWqqiqdc845ysnJUWtra4+HBQAAkS822E+YMWOGZsyY0e0xY4wee+wx/fa3v9V1110nSfrLX/4il8uljRs36sYbb+zZtAAAIOKF9DUfhw4dktfrVXZ2tn9fUlKSpkyZooqKilB+KQAAEKGCfubjVLxeryTJ5XIF7He5XP5j3+Tz+eTz+fwft7S0hHIkAABwlgn73S5FRUVKSkryb+np6eEeCQAA9KKQxofb7ZYkNTY2BuxvbGz0H/umwsJCNTc3+7f6+vpQjgQAAM4yIY2PUaNGye12q6yszL+vpaVFVVVV8ng83X6Ow+GQ0+kM2AAAQN8V9Gs+Tpw4oYMHD/o/PnTokGpqapSSkqKMjAwtXrxYf/jDH3TBBRdo1KhRuv/++5WWlqbrr78+lHMDAIAIFXR87N69Wz/4wQ/8HxcUFEiS8vLytHbtWt1zzz06efKkbr/9djU1NWnatGnasmWL4uPjQzc1AACIWEHHx1VXXSVjzLcej4qK0kMPPaSHHnqoR4MBAIC+Kex3uwAAgP6F+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFbFhnsA9GHvvRfuCYIzZIiUkRHuKQCgzyM+EHJDT3yqjqgoxdx0U7hHCUpHwkDFvP8eAQIAvYz4QMg5fScUY4wW/eQuHUxND/c4p2X0J/V6fPOfpKNHiQ8A6GXEB3rNwdR07XePDvcYAICzDC84BQAAVhEfAADAKn7tAgBAT9TVffl6sUgS5rv7iA8AAM5UXZ06Mscq5vPPwj1JUMJ9dx/xAQDAmTp6VDGff8bdfUHqtfgoLi7WI488Iq/Xq4kTJ+rJJ5/U5MmTe+vLAQAQNtzdF5xeecHpCy+8oIKCAi1btkxvvfWWJk6cqJycHB05cqQ3vhwAAIggvRIfjz76qG677TbNmzdPF154oZ566ikNHDhQzz77bG98OQAAEEFC/muXtrY2VVdXq7Cw0L8vOjpa2dnZqqio6HK+z+eTz+fzf9zc3CxJamlpCfVo0okTkqR070G1t7WG/vq9YOgn9WoRM/e29GP/VoskVVf7/38SEaKjpc7OcE8RHGa2JxLnjrSZa2slRei/dydOSCH8WfvVz21jzP8+2YTYxx9/bCSZN998M2D/3XffbSZPntzl/GXLlhlJbGxsbGxsbH1gq6+v/5+tEPa7XQoLC1VQUOD/uLOzU8eOHVNqaqqioqKCulZLS4vS09NVX18vp9MZ6lEjEmvSPdalK9ake6xL91iXrvr7mhhjdPz4caWlpf3Pc0MeH0OGDFFMTIwaGxsD9jc2Nsrtdnc53+FwyOFwBOxLTk7u0QxOp7Nf/g9/KqxJ91iXrliT7rEu3WNduurPa5KUlHRa54X8BadxcXHKyspSWVmZf19nZ6fKysrk8XhC/eUAAECE6ZVfuxQUFCgvL0+TJk3S5MmT9dhjj+nkyZOaN29eb3w5AAAQQXolPm644Qb95z//0QMPPCCv16vvfe972rJli1wuV298OT+Hw6Fly5Z1+TVOf8aadI916Yo16R7r0j3WpSvW5PRFGXM698QAAACERq/8kTEAAIBvQ3wAAACriA8AAGAV8QEAAKzqM/FRXFys8847T/Hx8ZoyZYp27twZ7pF6TVFRkS699FINGjRIw4YN0/XXX6/a/76/wFdaW1uVn5+v1NRUJSYmKjc3t8sffqurq9PMmTM1cOBADRs2THfffbe++OILmw+lV61YsUJRUVFavHixf19/XJePP/5YN910k1JTU5WQkKDx48dr9+7d/uPGGD3wwAMaPny4EhISlJ2drQMHDgRc49ixY5o7d66cTqeSk5M1f/58nYik98D5ho6ODt1///0aNWqUEhIS9J3vfEe///3vA96Toj+sy/bt2zVr1iylpaUpKipKGzduDDgeqjV45513dMUVVyg+Pl7p6el6+OGHe/uhnbFTrUl7e7uWLl2q8ePH65xzzlFaWppuueUWNTQ0BFyjr61Jr+j5u7mEX2lpqYmLizPPPvus2b9/v7nttttMcnKyaWxsDPdovSInJ8eUlJSYffv2mZqaGvPjH//YZGRkmBMnTvjPueOOO0x6eropKyszu3fvNlOnTjWXXXaZ//gXX3xhxo0bZ7Kzs82ePXvMK6+8YoYMGWIKCwvD8ZBCbufOnea8884zEyZMMIsWLfLv72/rcuzYMTNy5Ehz6623mqqqKvPhhx+a1157zRw8eNB/zooVK0xSUpLZuHGjefvtt821115rRo0aZT7//HP/OT/60Y/MxIkTTWVlpfnHP/5hRo8ebebMmROOhxQSy5cvN6mpqWbz5s3m0KFDZv369SYxMdE8/vjj/nP6w7q88sor5r777jMvvfSSkWQ2bNgQcDwUa9Dc3GxcLpeZO3eu2bdvn3n++edNQkKCefrpp209zKCcak2amppMdna2eeGFF8z7779vKioqzOTJk01WVlbANframvSGPhEfkydPNvn5+f6POzo6TFpamikqKgrjVPYcOXLESDLl5eXGmC+/QQYMGGDWr1/vP+e9994zkkxFRYUx5stvsOjoaOP1ev3nrF692jidTuPz+ew+gBA7fvy4ueCCC8zWrVvN97//fX989Md1Wbp0qZk2bdq3Hu/s7DRut9s88sgj/n1NTU3G4XCY559/3hhjzLvvvmskmV27dvnPefXVV01UVJT5+OOPe2/4XjRz5kzzi1/8ImDf7Nmzzdy5c40x/XNdvvmDNlRrsGrVKjN48OCA75+lS5eaMWPG9PIj6rnuguybdu7caSSZjz76yBjT99ckVCL+1y5tbW2qrq5Wdna2f190dLSys7NVUVERxsnsaW5uliSlpKRIkqqrq9Xe3h6wJpmZmcrIyPCvSUVFhcaPHx/wh99ycnLU0tKi/fv3W5w+9PLz8zVz5syAxy/1z3X529/+pkmTJulnP/uZhg0bposvvljPPPOM//ihQ4fk9XoD1iQpKUlTpkwJWJPk5GRNmjTJf052draio6NVVVVl78GE0GWXXaaysjJ98MEHkqS3335bO3bs0IwZMyT133X5ulCtQUVFha688krFxcX5z8nJyVFtba0+/fRTS4+m9zQ3NysqKsr/nmSsyekJ+7va9tTRo0fV0dHR5a+nulwuvf/++2Gayp7Ozk4tXrxYl19+ucaNGydJ8nq9iouL6/IGfS6XS16v139Od2v21bFIVVpaqrfeeku7du3qcqw/rsuHH36o1atXq6CgQL/5zW+0a9cu/epXv1JcXJzy8vL8j6m7x/z1NRk2bFjA8djYWKWkpETkmkjSvffeq5aWFmVmZiomJkYdHR1avny55s6dK0n9dl2+LlRr4PV6NWrUqC7X+OrY4MGDe2V+G1pbW7V06VLNmTPH/0Zy/X1NTlfEx0d/l5+fr3379mnHjh3hHiXs6uvrtWjRIm3dulXx8fHhHues0NnZqUmTJumPf/yjJOniiy/Wvn379NRTTykvLy/M04XPiy++qOeee07r1q3TRRddpJqaGi1evFhpaWn9el1w+trb2/Xzn/9cxhitXr063ONEnIj/tcuQIUMUExPT5Y6FxsZGud3uME1lx4IFC7R582a9/vrrGjFihH+/2+1WW1ubmpqaAs7/+pq43e5u1+yrY5GourpaR44c0SWXXKLY2FjFxsaqvLxcTzzxhGJjY+VyufrdugwfPlwXXnhhwL6xY8eqrq5O0v8/plN9/7jdbh05ciTg+BdffKFjx45F5JpI0t133617771XN954o8aPH6+bb75ZS5YsUVFRkaT+uy5fF6o16GvfU9L/h8dHH32krVu3+p/1kPrvmgQr4uMjLi5OWVlZKisr8+/r7OxUWVmZPB5PGCfrPcYYLViwQBs2bNC2bdu6PH2XlZWlAQMGBKxJbW2t6urq/Gvi8Xi0d+/egG+Sr76JvvnDKlJcffXV2rt3r2pqavzbpEmTNHfuXP9/97d1ufzyy7vchv3BBx9o5MiRkqRRo0bJ7XYHrElLS4uqqqoC1qSpqUnV1dX+c7Zt26bOzk5NmTLFwqMIvc8++0zR0YH//MXExKizs1NS/12XrwvVGng8Hm3fvl3t7e3+c7Zu3aoxY8ZE5K8XvgqPAwcO6O9//7tSU1MDjvfHNTkj4X7FayiUlpYah8Nh1q5da959911z++23m+Tk5IA7FvqSO++80yQlJZk33njDHD582L999tln/nPuuOMOk5GRYbZt22Z2795tPB6P8Xg8/uNf3VJ6zTXXmJqaGrNlyxYzdOjQiL2l9Nt8/W4XY/rfuuzcudPExsaa5cuXmwMHDpjnnnvODBw40Pz1r3/1n7NixQqTnJxsXn75ZfPOO++Y6667rtvbKS+++GJTVVVlduzYYS644IKIuqX0m/Ly8sy5557rv9X2pZdeMkOGDDH33HOP/5z+sC7Hjx83e/bsMXv27DGSzKOPPmr27Nnjv3MjFGvQ1NRkXC6Xufnmm82+fftMaWmpGThw4Fl7W+mp1qStrc1ce+21ZsSIEaampibg39+v37nS19akN/SJ+DDGmCeffNJkZGSYuLg4M3nyZFNZWRnukXqNpG63kpIS/zmff/65+eUvf2kGDx5sBg4caH7605+aw4cPB1znX//6l5kxY4ZJSEgwQ4YMMXfddZdpb2+3/Gh61zfjoz+uy6ZNm8y4ceOMw+EwmZmZ5s9//nPA8c7OTnP//fcbl8tlHA6Hufrqq01tbW3AOZ988omZM2eOSUxMNE6n08ybN88cP37c5sMIqZaWFrNo0SKTkZFh4uPjzfnnn2/uu+++gB8g/WFdXn/99W7/LcnLyzPGhG4N3n77bTNt2jTjcDjMueeea1asWGHrIQbtVGty6NChb/339/XXX/dfo6+tSW+IMuZrf9IPAACgl0X8az4AAEBkIT4AAIBVxAcAALCK+AAAAFYRHwAAwCriAwAAWEV8AAAAq4gPAABgFfEBAACsIj4AAIBVxAcAALCK+AAAAFb9H5QeGni2yTDdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot token lengths for q&a data\n",
    "tok_len_data = [num_tokens(ans) for _, ans in qa_data]\n",
    "\n",
    "plt.hist(tok_len_data, edgecolor=\"red\", bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 q&a sections split into 82 strings.\n"
     ]
    }
   ],
   "source": [
    "# split sections into chunks\n",
    "\n",
    "MAX_TOKENS = 1600\n",
    "qa_strings = []\n",
    "\n",
    "for section in qa_data:\n",
    "    qa_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(qa_data)} q&a sections split into {len(qa_strings)} strings.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a temperature of 40 dangerous?\n",
      "\n",
      "\n",
      "When your child has a fever, the most important thing is how your child looks and acts, not the number on the thermometer. For example, when the fever is high, your child will probably look more sick and feel very tired. When the fever is better, your child  should be more active, playful and able to drink fluids.\n",
      "Many parents worry if their child’s temperature is higher, for example, 39° or 40°, that it could mean something more dangerous is going on. Many children with a cold or flu develop a fever of 39° or 40°.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "print(qa_strings[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed document chunks\n",
    "\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n"
     ]
    }
   ],
   "source": [
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(qa_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = qa_strings[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({\"text\": qa_strings, \"embedding\": embeddings})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store document chunks and embeddings\n",
    "\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "\n",
    "(For larger datasets, use a vector database, which will be more performant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "\n",
    "SAVE_PATH = \"db/embedded_qa_001.csv\"\n",
    "\n",
    "df.to_csv(SAVE_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
